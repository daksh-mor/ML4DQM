{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML4SCI at GSoC 2025 – ML4DQM Evaluation Test\n",
    "## Data Understanding and Exploration\n",
    "\n",
    "This notebook focuses on understanding the HCAL DigiOccupancy datasets and visualizing key features that will be important for the classification task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task Overview\n",
    "\n",
    "We are provided with two synthetic datasets containing DigiOccupancy values from the Hadronic Calorimeter (HCAL) at the CMS detector:\n",
    "- Run355456_Dataset.npy\n",
    "- Run357479_Dataset.npy\n",
    "\n",
    "Our objective is to develop a Vision Transformer (ViT) model to classify these \"images\" according to which run they originated from."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Description\n",
    "\n",
    "The datasets represent DigiOccupancy (hit multiplicity) values for the Hadronic Calorimeter (HCAL) at the CMS detector. Each dataset has the shape (10000, 64, 72), where:\n",
    "\n",
    "- 10,000 refers to the number of luminosity sections (LS)\n",
    "- 64 refers to the number of iEta cells (pseudorapidity)\n",
    "- 72 refers to the number of iPhi cells (azimuthal angle)\n",
    "\n",
    "Each value in the array represents the number of particle hits detected in a specific cell during a specific luminosity section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LogNorm\n",
    "import seaborn as sns\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set the style for plots\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_context(\"notebook\", font_scale=1.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading the Datasets\n",
    "\n",
    "First, let's download and load the datasets. For this notebook, we'll need to ensure we have the datasets in our working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to download the datasets if not already present\n",
    "def download_dataset(url, filename):\n",
    "    if not os.path.exists(filename):\n",
    "        print(f\"Downloading {filename}...\")\n",
    "        try:\n",
    "            import urllib.request\n",
    "            urllib.request.urlretrieve(url, filename)\n",
    "            print(f\"Downloaded {filename} successfully.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error downloading {filename}: {e}\")\n",
    "            print(f\"Please download {filename} manually from the provided URL.\")\n",
    "    else:\n",
    "        print(f\"{filename} already exists.\")\n",
    "\n",
    "# URLs for the datasets\n",
    "url1 = \"https://cernbox.cern.ch/s/cDOFb5myDHGqRfc\"\n",
    "url2 = \"https://cernbox.cern.ch/s/n8NvyK2ldUPUxa9\"\n",
    "\n",
    "# Download the datasets\n",
    "download_dataset(url1, \"Run355456_Dataset.npy\")\n",
    "download_dataset(url2, \"Run357479_Dataset.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the datasets\n",
    "try:\n",
    "    run1 = np.load(\"Run355456_Dataset.npy\")\n",
    "    run2 = np.load(\"Run357479_Dataset.npy\")\n",
    "    print(f\"Run1 shape: {run1.shape}\")\n",
    "    print(f\"Run2 shape: {run2.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: Dataset files not found. Please ensure they are in the current directory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Understanding HCAL Coordinates\n",
    "\n",
    "Before diving into the data analysis, let's understand what iEta and iPhi represent in the HCAL detector:\n",
    "\n",
    "### iEta (η) - Pseudorapidity\n",
    "- Related to the polar angle θ by η = -ln(tan(θ/2))\n",
    "- Measures position along the beam axis\n",
    "- Invariant under Lorentz boosts along the beam axis\n",
    "- In our dataset: 64 discrete iEta values\n",
    "\n",
    "### iPhi (φ) - Azimuthal Angle\n",
    "- Measures angular position around the beam pipe\n",
    "- Covers the full 360° around the cylindrical detector\n",
    "- In our dataset: 72 discrete iPhi values\n",
    "\n",
    "The following diagram illustrates how these coordinates map to the physical HCAL detector:\n",
    "\n",
    "```\n",
    "                      ^ iEta\n",
    "                      |\n",
    "                      |\n",
    "    +----------------+----------------+\n",
    "    |                |                |\n",
    "    |                |                |\n",
    "    |                |                |\n",
    "    |                |                |\n",
    "    |                |                |\n",
    "    +----------------+----------------+ --> iPhi\n",
    "    |                |                |\n",
    "    |                |                |\n",
    "    |                |                |\n",
    "    |                |                |\n",
    "    |                |                |\n",
    "    +----------------+----------------+\n",
    "                      |\n",
    "                      |\n",
    "```\n",
    "\n",
    "Each cell in this grid represents a detector cell, and the DigiOccupancy value is the number of particle hits in that cell during a luminosity section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Basic Data Statistics\n",
    "\n",
    "Let's calculate some basic statistics about the datasets to understand their characteristics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_statistics(data, name):\n",
    "    \"\"\"Calculate and print basic statistics for a dataset\"\"\"\n",
    "    print(f\"\\n{name} Statistics:\")\n",
    "    print(f\"Min value: {np.min(data)}\")\n",
    "    print(f\"Max value: {np.max(data)}\")\n",
    "    print(f\"Mean value: {np.mean(data):.2f}\")\n",
    "    print(f\"Median value: {np.median(data):.2f}\")\n",
    "    print(f\"Standard deviation: {np.std(data):.2f}\")\n",
    "    \n",
    "    # Calculate the percentage of zero values\n",
    "    zero_percentage = (data == 0).sum() / data.size * 100\n",
    "    print(f\"Percentage of zero values: {zero_percentage:.2f}%\")\n",
    "    \n",
    "    # Calculate sparsity of each image\n",
    "    sparsity_per_image = [(img == 0).sum() / img.size * 100 for img in data]\n",
    "    print(f\"Average image sparsity: {np.mean(sparsity_per_image):.2f}%\")\n",
    "    print(f\"Min image sparsity: {np.min(sparsity_per_image):.2f}%\")\n",
    "    print(f\"Max image sparsity: {np.max(sparsity_per_image):.2f}%\")\n",
    "\n",
    "# Calculate statistics for both runs\n",
    "calculate_statistics(run1, \"Run 355456\")\n",
    "calculate_statistics(run2, \"Run 357479\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Key Visualizations\n",
    "\n",
    "Now let's create some visualizations to better understand the data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Average Occupancy Maps\n",
    "\n",
    "First, let's look at the average DigiOccupancy for each cell across all luminosity sections. This will show us which regions of the detector are typically active."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average maps\n",
    "avg_run1 = np.mean(run1, axis=0)\n",
    "avg_run2 = np.mean(run2, axis=0)\n",
    "\n",
    "# Create the figure\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 7))\n",
    "\n",
    "# Find global min and max for consistent color scale, excluding zeros\n",
    "vmin = min(np.min(avg_run1[avg_run1 > 0]), np.min(avg_run2[avg_run2 > 0]))\n",
    "vmax = max(np.max(avg_run1), np.max(avg_run2))\n",
    "\n",
    "# Plot average maps with logarithmic scale\n",
    "im1 = axes[0].imshow(avg_run1, cmap='viridis', \n",
    "                     norm=LogNorm(vmin=max(0.1, vmin), vmax=vmax))\n",
    "axes[0].set_title(\"Average DigiOccupancy - Run 355456\", fontsize=14)\n",
    "axes[0].set_xlabel(\"iPhi (azimuthal angle)\", fontsize=12)\n",
    "axes[0].set_ylabel(\"iEta (pseudorapidity)\", fontsize=12)\n",
    "\n",
    "im2 = axes[1].imshow(avg_run2, cmap='viridis', \n",
    "                     norm=LogNorm(vmin=max(0.1, vmin), vmax=vmax))\n",
    "axes[1].set_title(\"Average DigiOccupancy - Run 357479\", fontsize=14)\n",
    "axes[1].set_xlabel(\"iPhi (azimuthal angle)\", fontsize=12)\n",
    "axes[1].set_ylabel(\"iEta (pseudorapidity)\", fontsize=12)\n",
    "\n",
    "# Add a colorbar\n",
    "cbar = fig.colorbar(im1, ax=axes, label='Average DigiOccupancy (log scale)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('average_occupancy_maps.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Difference Map\n",
    "\n",
    "Next, let's visualize the difference between the average occupancy maps to see which regions differ most between the two runs. This will highlight the distinguishing features for our classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate difference\n",
    "diff = avg_run2 - avg_run1\n",
    "\n",
    "# Create figure\n",
    "plt.figure(figsize=(12, 9))\n",
    "\n",
    "# Set symmetric color limits for better visualization\n",
    "vmax = max(abs(np.max(diff)), abs(np.min(diff)))\n",
    "\n",
    "# Plot difference map with diverging color scale\n",
    "im = plt.imshow(diff, cmap='RdBu_r', vmin=-vmax, vmax=vmax)\n",
    "plt.title(\"Difference in DigiOccupancy (Run 357479 - Run 355456)\", fontsize=16)\n",
    "plt.xlabel(\"iPhi (azimuthal angle)\", fontsize=14)\n",
    "plt.ylabel(\"iEta (pseudorapidity)\", fontsize=14)\n",
    "\n",
    "# Add colorbar\n",
    "cbar = plt.colorbar(im, label='Difference in DigiOccupancy')\n",
    "\n",
    "# Add annotations for positive and negative regions\n",
    "plt.text(5, 5, \"Blue: Higher in Run 355456\", color='blue', fontsize=12, \n",
    "         bbox={\"facecolor\":\"white\", \"alpha\":0.8, \"pad\":5})\n",
    "plt.text(5, 10, \"Red: Higher in Run 357479\", color='red', fontsize=12, \n",
    "         bbox={\"facecolor\":\"white\", \"alpha\":0.8, \"pad\":5})\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('difference_map.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Sample Images from Each Run\n",
    "\n",
    "Let's look at a few random samples from each run to get a sense of what individual images look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a figure to display random samples\n",
    "num_samples = 3\n",
    "fig, axes = plt.subplots(num_samples, 2, figsize=(14, 4 * num_samples))\n",
    "\n",
    "# Select random indices\n",
    "np.random.seed(42)  # For reproducibility\n",
    "run1_indices = np.random.choice(run1.shape[0], num_samples, replace=False)\n",
    "run2_indices = np.random.choice(run2.shape[0], num_samples, replace=False)\n",
    "\n",
    "# Find global min and max for consistent color scale\n",
    "vmin = min(np.min(run1[run1 > 0]), np.min(run2[run2 > 0]))\n",
    "vmax = max(np.max(run1), np.max(run2))\n",
    "\n",
    "for i in range(num_samples):\n",
    "    # Plot Run 355456\n",
    "    im1 = axes[i, 0].imshow(run1[run1_indices[i]], \n",
    "                           cmap='viridis', \n",
    "                           norm=LogNorm(vmin=max(0.1, vmin), vmax=vmax))\n",
    "    axes[i, 0].set_title(f\"Run 355456 - Sample {run1_indices[i]}\")\n",
    "    axes[i, 0].set_xlabel(\"iPhi\")\n",
    "    axes[i, 0].set_ylabel(\"iEta\")\n",
    "    \n",
    "    # Plot Run 357479\n",
    "    im2 = axes[i, 1].imshow(run2[run2_indices[i]], \n",
    "                           cmap='viridis', \n",
    "                           norm=LogNorm(vmin=max(0.1, vmin), vmax=vmax))\n",
    "    axes[i, 1].set_title(f\"Run 357479 - Sample {run2_indices[i]}\")\n",
    "    axes[i, 1].set_xlabel(\"iPhi\")\n",
    "    axes[i, 1].set_ylabel(\"iEta\")\n",
    "\n",
    "# Add colorbar\n",
    "fig.colorbar(im1, ax=axes, label='DigiOccupancy Value (log scale)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('sample_images.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Sparsity Analysis\n",
    "\n",
    "The dataset description mentions that there will be many zero-valued entries. Let's analyze the sparsity patterns to understand where hits typically occur in the detector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create binary masks where values are present (non-zero)\n",
    "mask_run1 = np.mean(run1 > 0, axis=0)\n",
    "mask_run2 = np.mean(run2 > 0, axis=0)\n",
    "\n",
    "# Difference in occurrence frequency\n",
    "diff_mask = mask_run2 - mask_run1\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# Plot masks\n",
    "im1 = axes[0].imshow(mask_run1, cmap='Blues', vmin=0, vmax=1)\n",
    "axes[0].set_title(\"Non-Zero Frequency - Run 355456\")\n",
    "axes[0].set_xlabel(\"iPhi\")\n",
    "axes[0].set_ylabel(\"iEta\")\n",
    "fig.colorbar(im1, ax=axes[0], label='Frequency of Non-Zero Values')\n",
    "\n",
    "im2 = axes[1].imshow(mask_run2, cmap='Blues', vmin=0, vmax=1)\n",
    "axes[1].set_title(\"Non-Zero Frequency - Run 357479\") \n",
    "axes[1].set_xlabel(\"iPhi\")\n",
    "axes[1].set_ylabel(\"iEta\")\n",
    "fig.colorbar(im2, ax=axes[1], label='Frequency of Non-Zero Values')\n",
    "\n",
    "# Plot difference\n",
    "vmax_diff = max(abs(np.min(diff_mask)), abs(np.max(diff_mask)))\n",
    "im3 = axes[2].imshow(diff_mask, cmap='RdBu_r', vmin=-vmax_diff, vmax=vmax_diff)\n",
    "axes[2].set_title(\"Difference in Non-Zero Frequency\") \n",
    "axes[2].set_xlabel(\"iPhi\")\n",
    "axes[2].set_ylabel(\"iEta\")\n",
    "fig.colorbar(im3, ax=axes[2], label='Frequency Difference')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('sparsity_pattern.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Value Distribution Analysis\n",
    "\n",
    "Let's look at the distribution of non-zero DigiOccupancy values to understand the range and frequency of hit counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Flatten the arrays for histograms, excluding zeros\n",
    "run1_flat = run1[run1 > 0].flatten()\n",
    "run2_flat = run2[run2 > 0].flatten()\n",
    "\n",
    "# If datasets are large, sample for better performance\n",
    "sample_size = 100000\n",
    "if len(run1_flat) > sample_size:\n",
    "    run1_flat = np.random.choice(run1_flat, sample_size, replace=False)\n",
    "if len(run2_flat) > sample_size:\n",
    "    run2_flat = np.random.choice(run2_flat, sample_size, replace=False)\n",
    "\n",
    "# Create histograms with log scale on y-axis\n",
    "sns.histplot(run1_flat, color='blue', label='Run 355456', alpha=0.5, \n",
    "             log_scale=(False, True), stat='density', bins=50)\n",
    "sns.histplot(run2_flat, color='orange', label='Run 357479', alpha=0.5, \n",
    "             log_scale=(False, True), stat='density', bins=50)\n",
    "\n",
    "plt.xlabel('DigiOccupancy Value')\n",
    "plt.ylabel('Density (log scale)')\n",
    "plt.title('Distribution of Non-Zero DigiOccupancy Values')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig('value_distribution.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary of Findings\n",
    "\n",
    "Based on our exploratory data analysis, we can summarize our findings about the HCAL DigiOccupancy datasets:\n",
    "\n",
    "1. **Data Structure**: Each dataset contains 10,000 images of size 64×72 representing hits in the HCAL detector.\n",
    "\n",
    "2. **Sparsity**: The data is highly sparse, with a significant percentage of zero values, which is expected as particle hits tend to be localized.\n",
    "\n",
    "3. **Value Distribution**: Non-zero values have a wide dynamic range, with most values being low but some high-activity regions showing much larger counts.\n",
    "\n",
    "4. **Spatial Patterns**: The average occupancy maps show clear spatial patterns of activity in the detector, with certain regions consistently showing higher hit counts.\n",
    "\n",
    "5. **Distinguishing Features**: The difference map reveals regions where the two runs differ significantly, which will be important for our classification task.\n",
    "\n",
    "6. **Classification Challenge**: Our task is to develop a Vision Transformer model that can identify which run a given DigiOccupancy image came from based on these patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Next Steps\n",
    "\n",
    "With this understanding of the data, we can now proceed to develop our Vision Transformer model. In the next notebook, we will:\n",
    "\n",
    "1. Preprocess the data for model training\n",
    "2. Implement a Vision Transformer architecture\n",
    "3. Train the model on our classification task\n",
    "4. Evaluate its performance using accuracy, ROC curves, and AUC\n",
    "\n",
    "The key insights from this exploratory analysis will inform our modeling approach, particularly with respect to handling the sparse nature of the data and focusing on the differentiating features between the two runs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
