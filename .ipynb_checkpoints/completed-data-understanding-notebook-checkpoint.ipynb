{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML4SCI at GSoC 2025 – ML4DQM Evaluation Test\n",
    "## Data Understanding and Exploration (Fixed)\n",
    "\n",
    "This notebook focuses on understanding the HCAL DigiOccupancy datasets and visualizing key features that will be important for the Vision Transformer classification task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task Overview\n",
    "\n",
    "We are provided with two synthetic datasets containing DigiOccupancy values from the Hadronic Calorimeter (HCAL) at the CMS detector:\n",
    "- Run355456_Dataset.npy\n",
    "- Run357479_Dataset.npy\n",
    "\n",
    "Our objective is to develop a Vision Transformer (ViT) model to classify these \"images\" according to which run they originated from."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Description\n",
    "\n",
    "The datasets represent DigiOccupancy (hit multiplicity) values for the Hadronic Calorimeter (HCAL) at the CMS detector. Each dataset has the shape (10000, 64, 72), where:\n",
    "\n",
    "- 10,000 refers to the number of luminosity sections (LS)\n",
    "- 64 refers to the number of iEta cells (pseudorapidity)\n",
    "- 72 refers to the number of iPhi cells (azimuthal angle)\n",
    "\n",
    "Each value in the array represents the number of particle hits detected in a specific cell during a specific luminosity section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LogNorm\n",
    "import seaborn as sns\n",
    "import os\n",
    "# from tqdm import tqdm # tqdm was imported but not used\n",
    "\n",
    "# Set the style for plots\n",
    "try:\n",
    "    plt.style.use('seaborn-v0_8-whitegrid')\n",
    "except OSError:\n",
    "    print(\"Seaborn style 'seaborn-v0_8-whitegrid' not found. Using default.\")\n",
    "    # Or use an alternative style: plt.style.use('seaborn-whitegrid') or plt.style.use('ggplot')\n",
    "sns.set_context(\"notebook\", font_scale=1.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Understanding HCAL Coordinates\n",
    "\n",
    "Before diving into the data analysis, let's understand what iEta and iPhi represent in the HCAL detector:\n",
    "\n",
    "### iEta (η) - Pseudorapidity\n",
    "- Related to the polar angle θ by η = -ln(tan(θ/2))\n",
    "- Measures position along the beam axis\n",
    "- Invariant under Lorentz boosts along the beam axis\n",
    "- In our dataset: 64 discrete iEta values\n",
    "\n",
    "### iPhi (φ) - Azimuthal Angle\n",
    "- Measures angular position around the beam pipe\n",
    "- Covers the full 360° around the cylindrical detector\n",
    "- In our dataset: 72 discrete iPhi values (5° per division)\n",
    "\n",
    "Unlike standard cylindrical coordinates, HCAL doesn't use the direct radial distance as a primary coordinate because the detector has fixed radial layers. The primary focus of the segmentation is the angular position rather than the radial depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Data Loading Section ---\n",
    "\n",
    "# NOTE: The automatic download function below is commented out because\n",
    "#       `urlretrieve` usually does not work directly with CERNBox share links.\n",
    "#\n",
    "#       >>> PLEASE DOWNLOAD THE FILES MANUALLY <<<\n",
    "#       1. Go to: https://cernbox.cern.ch/s/cDOFb5myDHGqRfc -> Download Run355456_Dataset.npy\n",
    "#       2. Go to: https://cernbox.cern.ch/s/n8NvyK2ldUPUxa9 -> Download Run357479_Dataset.npy\n",
    "#       3. Place both downloaded .npy files in the SAME DIRECTORY as this script/notebook.\n",
    "\n",
    "# # Function to download the datasets if not already present (Commented Out)\n",
    "# def download_dataset(url, filename):\n",
    "#     if not os.path.exists(filename):\n",
    "#         print(f\"Attempting to download {filename} from {url}...\")\n",
    "#         print(\"NOTE: This automatic download might fail. If it does, please download manually.\")\n",
    "#         try:\n",
    "#             import urllib.request\n",
    "#             # This line is the likely point of failure for CERNBox links\n",
    "#             urllib.request.urlretrieve(url, filename)\n",
    "#             print(f\"Downloaded {filename} successfully.\")\n",
    "#             # Basic check: Ensure file size is reasonable (e.g., > 1MB)\n",
    "#             if os.path.getsize(filename) < 1024*1024:\n",
    "#                 print(f\"Warning: Downloaded file {filename} seems too small. Manual download recommended.\")\n",
    "#                 # os.remove(filename) # Optionally remove potentially bad download\n",
    "#                 # raise Exception(\"Downloaded file too small.\")\n",
    "#         except Exception as e:\n",
    "#             print(f\"Error downloading {filename}: {e}\")\n",
    "#             print(f\"Please download {filename} manually from the URL and place it here.\")\n",
    "#             # Ensure partial downloads are removed if necessary\n",
    "#             if os.path.exists(filename):\n",
    "#                 try:\n",
    "#                     os.remove(filename)\n",
    "#                 except OSError:\n",
    "#                     pass # Ignore if removal fails\n",
    "#             raise FileNotFoundError(f\"Manual download required for {filename}\") from e\n",
    "#     else:\n",
    "#         print(f\"{filename} already exists.\")\n",
    "\n",
    "# URLs for the datasets (kept for reference)\n",
    "# url1 = \"https://cernbox.cern.ch/s/cDOFb5myDHGqRfc\" # Link for Run355456_Dataset.npy\n",
    "# url2 = \"https://cernbox.cern.ch/s/n8NvyK2ldUPUxa9\" # Link for Run357479_Dataset.npy\n",
    "filename1 = \"Run355456_Dataset.npy\"\n",
    "filename2 = \"Run357479_Dataset.npy\"\n",
    "\n",
    "# # Attempt to download the datasets (Commented Out - Manual Download Required)\n",
    "# try:\n",
    "#     download_dataset(url1, filename1)\n",
    "#     download_dataset(url2, filename2)\n",
    "# except Exception as e:\n",
    "#     print(f\"\\nHalting execution due to download issue: {e}\")\n",
    "#     # Exit or handle appropriately if in a script; in a notebook, execution stops here.\n",
    "#     # exit() # Or raise the exception again depending on desired flow\n",
    "\n",
    "# Load the datasets (assuming manual download)\n",
    "print(f\"Attempting to load data from {filename1} and {filename2}...\")\n",
    "print(\"Ensure these files have been manually downloaded and are in the current directory.\")\n",
    "try:\n",
    "    run1 = np.load(filename1)\n",
    "    run2 = np.load(filename2)\n",
    "    print(f\"\\nSuccessfully loaded datasets:\")\n",
    "    print(f\"Run1 shape: {run1.shape}, dtype: {run1.dtype}\")\n",
    "    print(f\"Run2 shape: {run2.shape}, dtype: {run2.dtype}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"\\nError: Dataset file(s) not found.\")\n",
    "    print(f\"Please ensure '{filename1}' and '{filename2}' are downloaded manually\")\n",
    "    print(\"and placed in the same directory as this notebook/script.\")\n",
    "    # Stop execution if files aren't loaded, as subsequent cells depend on them.\n",
    "    # In a notebook, raising an error stops the current cell execution.\n",
    "    raise FileNotFoundError(\"Cannot proceed without data files. Please download manually.\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nAn unexpected error occurred while loading the numpy files: {e}\")\n",
    "    print(\"The files might be corrupted or not valid NumPy arrays.\")\n",
    "    raise RuntimeError(\"Cannot proceed due to data loading error.\") from e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Basic Data Statistics\n",
    "\n",
    "Let's calculate some basic statistics about the datasets to understand their characteristics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_statistics(data, name):\n",
    "    \"\"\"Calculate and print basic statistics for a dataset\"\"\"\n",
    "    print(f\"\\n{name} Statistics:\")\n",
    "    if data.size == 0:\n",
    "        print(\"  Dataset is empty!\")\n",
    "        return\n",
    "    print(f\"  Min value: {np.min(data)}\")\n",
    "    print(f\"  Max value: {np.max(data)}\")\n",
    "    print(f\"  Mean value: {np.mean(data):.2f}\")\n",
    "    print(f\"  Median value: {np.median(data):.2f}\")\n",
    "    print(f\"  Standard deviation: {np.std(data):.2f}\")\n",
    "\n",
    "    # Calculate the percentage of zero values\n",
    "    zero_percentage = (data == 0).sum() / data.size * 100\n",
    "    print(f\"  Percentage of zero values: {zero_percentage:.2f}%\")\n",
    "\n",
    "    # Calculate sparsity of each image (this can be slow for large datasets)\n",
    "    # print(f\"  Calculating image sparsity (may take a moment)...\")\n",
    "    # sparsity_per_image = [(img == 0).sum() / img.size * 100 for img in data]\n",
    "    # print(f\"  Average image sparsity: {np.mean(sparsity_per_image):.2f}%\")\n",
    "    # print(f\"  Min image sparsity: {np.min(sparsity_per_image):.2f}%\")\n",
    "    # print(f\"  Max image sparsity: {np.max(sparsity_per_image):.2f}%\")\n",
    "    # Optimization: Calculate average sparsity directly from zero_percentage if needed,\n",
    "    # but the per-image calculation gives min/max range.\n",
    "    # The global zero percentage already gives a good idea of overall sparsity.\n",
    "\n",
    "\n",
    "# Calculate statistics for both runs\n",
    "calculate_statistics(run1, \"Run 355456\")\n",
    "calculate_statistics(run2, \"Run 357479\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Average Occupancy Maps\n",
    "\n",
    "Let's visualize the average DigiOccupancy for each cell across all luminosity sections. This will show us which regions of the detector are typically active and reveal differences between the two runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nCalculating and plotting average occupancy maps...\")\n",
    "# Calculate average maps\n",
    "avg_run1 = np.mean(run1, axis=0)\n",
    "avg_run2 = np.mean(run2, axis=0)\n",
    "\n",
    "# Create the figure\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 7), sharey=True) # Share Y axis\n",
    "\n",
    "# Find global min and max for consistent color scale, excluding zeros for LogNorm\n",
    "# Handle cases where one run might have only zeros in some cells\n",
    "vmin = np.inf\n",
    "if np.any(avg_run1 > 0):\n",
    "    vmin = min(vmin, np.min(avg_run1[avg_run1 > 0]))\n",
    "if np.any(avg_run2 > 0):\n",
    "    vmin = min(vmin, np.min(avg_run2[avg_run2 > 0]))\n",
    "if not np.isfinite(vmin): # If both averages are all zero\n",
    "    vmin = 0.1 # Default minimum for LogNorm\n",
    "\n",
    "vmax = max(np.max(avg_run1), np.max(avg_run2))\n",
    "# Ensure vmin is less than vmax for LogNorm\n",
    "if vmin >= vmax:\n",
    "    vmin = max(0.1, vmax / 10) # Adjust vmin if necessary\n",
    "\n",
    "# Use a small positive value for vmin in LogNorm to avoid log(0) or log(negative) issues\n",
    "log_vmin = max(0.1, vmin)\n",
    "\n",
    "# Plot average maps with logarithmic scale\n",
    "im1 = axes[0].imshow(avg_run1, cmap='viridis',\n",
    "                     norm=LogNorm(vmin=log_vmin, vmax=vmax),\n",
    "                     aspect='auto') # Adjust aspect ratio\n",
    "axes[0].set_title(\"Average DigiOccupancy - Run 355456\", fontsize=14)\n",
    "axes[0].set_xlabel(\"iPhi (azimuthal angle)\", fontsize=12)\n",
    "axes[0].set_ylabel(\"iEta (pseudorapidity)\", fontsize=12)\n",
    "\n",
    "im2 = axes[1].imshow(avg_run2, cmap='viridis',\n",
    "                     norm=LogNorm(vmin=log_vmin, vmax=vmax),\n",
    "                     aspect='auto') # Adjust aspect ratio\n",
    "axes[1].set_title(\"Average DigiOccupancy - Run 357479\", fontsize=14)\n",
    "axes[1].set_xlabel(\"iPhi (azimuthal angle)\", fontsize=12)\n",
    "# axes[1].set_ylabel(\"iEta (pseudorapidity)\", fontsize=12) # Y label shared\n",
    "\n",
    "# Add a colorbar\n",
    "cbar = fig.colorbar(im1, ax=axes.ravel().tolist(), label='Average DigiOccupancy (log scale)', shrink=0.7)\n",
    "\n",
    "plt.suptitle(\"Average HCAL DigiOccupancy Maps\", fontsize=16, y=1.02)\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.98]) # Adjust layout to prevent title overlap\n",
    "# Construct a unique filename for saving\n",
    "save_filename_avg = 'average_occupancy_maps.png'\n",
    "print(f\"Saving average occupancy maps to {save_filename_avg}\")\n",
    "plt.savefig(save_filename_avg, dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Observations from Average Occupancy Maps:\n",
    "\n",
    "1. **Structured Activity Pattern**: Both runs show a very specific pattern of detector activity, with hits concentrated in two horizontal bands at approximately iEta positions 0-12 and 50-60.\n",
    "\n",
    "2. **Intensity Differences**: Run 357479 generally has higher DigiOccupancy values (more hits) than Run 355456, as indicated by the more yellow/green colors versus blue/purple.\n",
    "\n",
    "3. **Detector Structure**: The consistent pattern across both runs reveals the structural design of the HCAL detector, with active regions primarily at the edges of the iEta range.\n",
    "\n",
    "4. **Symmetry**: There's a symmetrical pattern between the top and bottom regions of the detector, likely corresponding to the forward and backward sections of the cylindrical detector.\n",
    "\n",
    "5. **High Sparsity**: Large portions of the detector (especially in the middle iEta region) show little to no activity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Difference Map\n",
    "\n",
    "Next, let's visualize the difference between the average occupancy maps to see which regions differ most between the two runs. This will highlight the distinguishing features for our classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nCalculating and plotting difference map...\")\n",
    "# Calculate difference\n",
    "diff = avg_run2 - avg_run1\n",
    "\n",
    "# Create figure\n",
    "plt.figure(figsize=(10, 8)) # Adjusted size slightly\n",
    "\n",
    "# Set symmetric color limits for better visualization\n",
    "vmax_diff = max(abs(np.max(diff)), abs(np.min(diff)))\n",
    "# Handle case where difference is zero everywhere\n",
    "if vmax_diff == 0:\n",
    "    vmax_diff = 1.0 # Avoid vmin=vmax=0\n",
    "\n",
    "# Plot difference map with diverging color scale\n",
    "im = plt.imshow(diff, cmap='RdBu_r', vmin=-vmax_diff, vmax=vmax_diff, aspect='auto')\n",
    "plt.title(\"Difference in Avg. DigiOccupancy (Run 357479 - Run 355456)\", fontsize=16)\n",
    "plt.xlabel(\"iPhi (azimuthal angle)\", fontsize=14)\n",
    "plt.ylabel(\"iEta (pseudorapidity)\", fontsize=14)\n",
    "\n",
    "# Add colorbar\n",
    "cbar = plt.colorbar(im, label='Difference in Average DigiOccupancy')\n",
    "\n",
    "# Add annotations for positive and negative regions (optional, can clutter)\n",
    "# plt.text(0.05, 0.95, \"Red: Higher in Run 357479\", color='red', fontsize=12,\n",
    "#          transform=plt.gca().transAxes, ha='left', va='top',\n",
    "#          bbox={\"facecolor\":\"white\", \"alpha\":0.8, \"pad\":5})\n",
    "# plt.text(0.05, 0.90, \"Blue: Higher in Run 355456\", color='blue', fontsize=12,\n",
    "#          transform=plt.gca().transAxes, ha='left', va='top',\n",
    "#          bbox={\"facecolor\":\"white\", \"alpha\":0.8, \"pad\":5})\n",
    "\n",
    "plt.tight_layout()\n",
    "save_filename_diff = 'difference_map.png'\n",
    "print(f\"Saving difference map to {save_filename_diff}\")\n",
    "plt.savefig(save_filename_diff, dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Observations from Difference Map:\n",
    "\n",
    "1. **Systematic Differences**: The difference map reveals a systematic pattern where Run 357479 consistently has higher hit counts (red) in the middle of the active bands (iEta around 12 and 50), while Run 355456 has higher counts (blue) at the edges of these bands.\n",
    "\n",
    "2. **Magnitude**: The difference can be quite substantial (over 400 hits in some regions) as shown by the color scale.\n",
    "\n",
    "3. **Spatial Pattern**: The differences follow a consistent pattern across all iPhi values, suggesting that the differences between runs affect the entire detector uniformly around the azimuthal angle.\n",
    "\n",
    "4. **Classification Features**: These systematic differences provide strong features for a Vision Transformer model to distinguish between the two runs.\n",
    "\n",
    "5. **Strategic Importance**: The difference map highlights exactly which regions of the detector a classification model should focus on to distinguish between the runs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Sample Images from Each Run\n",
    "\n",
    "Let's look at a few random samples from each run to get a sense of what individual images look like and how they vary within the same run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nPlotting sample images...\")\n",
    "# Set up a figure to display random samples\n",
    "num_samples = 3\n",
    "fig, axes = plt.subplots(num_samples, 2, figsize=(14, 4 * num_samples), sharex=True, sharey=True)\n",
    "\n",
    "# Select random indices\n",
    "np.random.seed(42)  # For reproducibility\n",
    "run1_indices = np.random.choice(run1.shape[0], num_samples, replace=False)\n",
    "run2_indices = np.random.choice(run2.shape[0], num_samples, replace=False)\n",
    "\n",
    "# Find global min and max across ALL samples for consistent color scale\n",
    "# This can be computationally expensive for many samples, using overall min/max is often sufficient\n",
    "# For LogNorm, find min excluding zeros across the relevant samples\n",
    "sampled_run1 = run1[run1_indices]\n",
    "sampled_run2 = run2[run2_indices]\n",
    "\n",
    "vmin_samples = np.inf\n",
    "if np.any(sampled_run1 > 0):\n",
    "    vmin_samples = min(vmin_samples, np.min(sampled_run1[sampled_run1 > 0]))\n",
    "if np.any(sampled_run2 > 0):\n",
    "    vmin_samples = min(vmin_samples, np.min(sampled_run2[sampled_run2 > 0]))\n",
    "if not np.isfinite(vmin_samples):\n",
    "    vmin_samples = 0.1 # Default\n",
    "\n",
    "vmax_samples = max(np.max(sampled_run1), np.max(sampled_run2))\n",
    "\n",
    "# Ensure vmin < vmax for LogNorm\n",
    "if vmin_samples >= vmax_samples:\n",
    "     vmin_samples = max(0.1, vmax_samples / 10)\n",
    "\n",
    "log_vmin_samples = max(0.1, vmin_samples)\n",
    "\n",
    "\n",
    "for i in range(num_samples):\n",
    "    # Plot Run 355456\n",
    "    im1 = axes[i, 0].imshow(run1[run1_indices[i]],\n",
    "                           cmap='viridis',\n",
    "                           norm=LogNorm(vmin=log_vmin_samples, vmax=vmax_samples),\n",
    "                           aspect='auto')\n",
    "    axes[i, 0].set_title(f\"Run 355456 - Sample LS Index {run1_indices[i]}\")\n",
    "    axes[i, 0].set_ylabel(\"iEta\")\n",
    "\n",
    "    # Plot Run 357479\n",
    "    im2 = axes[i, 1].imshow(run2[run2_indices[i]],\n",
    "                           cmap='viridis',\n",
    "                           norm=LogNorm(vmin=log_vmin_samples, vmax=vmax_samples),\n",
    "                           aspect='auto')\n",
    "    axes[i, 1].set_title(f\"Run 357479 - Sample LS Index {run2_indices[i]}\")\n",
    "\n",
    "    # Set x-label only on the bottom row\n",
    "    if i == num_samples - 1:\n",
    "        axes[i, 0].set_xlabel(\"iPhi\")\n",
    "        axes[i, 1].set_xlabel(\"iPhi\")\n",
    "\n",
    "\n",
    "# Add colorbar - using im1 as reference should be fine as norm is the same\n",
    "fig.colorbar(im1, ax=axes.ravel().tolist(), label='DigiOccupancy Value (log scale)', shrink=0.6)\n",
    "\n",
    "plt.suptitle(\"Sample HCAL DigiOccupancy Images per Run\", fontsize=16, y=1.0)\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.98])\n",
    "save_filename_samples = 'sample_images.png'\n",
    "print(f\"Saving sample images to {save_filename_samples}\")\n",
    "plt.savefig(save_filename_samples, dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Observations from Sample Images:\n",
    "\n",
    "1. **Individual Variation**: While maintaining the same general pattern seen in the average maps, individual samples show variation in intensity and specific details.\n",
    "\n",
    "2. **Consistent Features**: Despite this variation, the basic structure of active regions is highly consistent across samples from the same run.\n",
    "\n",
    "3. **Color/Intensity Difference**: Consistent with the average maps, Run 357479 samples generally show more yellow/green colors (higher values) compared to the blue/green colors in Run 355456.\n",
    "\n",
    "4. **Active Regions**: The activity is clearly concentrated in specific bands at the edges of the iEta range, with the middle region showing no activity.\n",
    "\n",
    "5. **Classification Challenge**: The samples demonstrate why this is a good machine learning task - there's a consistent pattern difference between runs, but also individual variation within runs that the model will need to account for."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Sparsity Analysis\n",
    "\n",
    "The dataset description mentions that there will be many zero-valued entries. Let's analyze the sparsity patterns to understand where hits typically occur in the detector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nAnalyzing and plotting sparsity patterns...\")\n",
    "# Calculate the frequency of non-zero values for each cell\n",
    "# (This is equivalent to np.mean(data > 0, axis=0))\n",
    "freq_nonzero_run1 = np.count_nonzero(run1, axis=0) / run1.shape[0]\n",
    "freq_nonzero_run2 = np.count_nonzero(run2, axis=0) / run2.shape[0]\n",
    "\n",
    "# Difference in occurrence frequency\n",
    "diff_freq = freq_nonzero_run2 - freq_nonzero_run1\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6), sharey=True)\n",
    "\n",
    "# Plot non-zero frequency maps\n",
    "im1 = axes[0].imshow(freq_nonzero_run1, cmap='Blues', vmin=0, vmax=1, aspect='auto')\n",
    "axes[0].set_title(\"Non-Zero Frequency - Run 355456\")\n",
    "axes[0].set_xlabel(\"iPhi\")\n",
    "axes[0].set_ylabel(\"iEta\")\n",
    "fig.colorbar(im1, ax=axes[0], label='Frequency of Non-Zero Values', shrink=0.7)\n",
    "\n",
    "im2 = axes[1].imshow(freq_nonzero_run2, cmap='Blues', vmin=0, vmax=1, aspect='auto')\n",
    "axes[1].set_title(\"Non-Zero Frequency - Run 357479\")\n",
    "axes[1].set_xlabel(\"iPhi\")\n",
    "# axes[1].set_ylabel(\"iEta\") # Shared Y\n",
    "fig.colorbar(im2, ax=axes[1], label='Frequency of Non-Zero Values', shrink=0.7)\n",
    "\n",
    "# Plot difference in frequency\n",
    "vmax_diff_freq = np.max(np.abs(diff_freq))\n",
    "if vmax_diff_freq == 0: vmax_diff_freq = 0.1 # Avoid zero range\n",
    "\n",
    "im3 = axes[2].imshow(diff_freq, cmap='RdBu_r', vmin=-vmax_diff_freq, vmax=vmax_diff_freq, aspect='auto')\n",
    "axes[2].set_title(\"Difference in Non-Zero Frequency\")\n",
    "axes[2].set_xlabel(\"iPhi\")\n",
    "# axes[2].set_ylabel(\"iEta\") # Shared Y\n",
    "fig.colorbar(im3, ax=axes[2], label='Frequency Difference\\n(Run2 - Run1)', shrink=0.7)\n",
    "\n",
    "plt.suptitle(\"Analysis of Cell Activity Frequency (Sparsity)\", fontsize=16, y=1.0)\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.98])\n",
    "save_filename_sparsity = 'sparsity_pattern.png'\n",
    "print(f\"Saving sparsity pattern plots to {save_filename_sparsity}\")\n",
    "plt.savefig(save_filename_sparsity, dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Observations from Sparsity Analysis:\n",
    "\n",
    "1. **Binary Activity**: The non-zero frequency maps show exactly where activity occurs regardless of intensity, with deep blue regions indicating cells that frequently have non-zero values.\n",
    "\n",
    "2. **Limited Active Areas**: Activity is highly concentrated in specific regions (iEta around 0-12 and 50-60), with almost no activity in the middle of the detector.\n",
    "\n",
    "3. **Similar Activity Patterns**: Both runs have very similar patterns of where activity occurs, with the difference map showing only subtle variations.\n",
    "\n",
    "4. **Vertical Striping**: The active regions show clear vertical stripes, indicating that for any given iPhi value, only specific iEta positions ever register hits.\n",
    "\n",
    "5. **Preprocessing Implications**: The consistent sparsity pattern suggests that we could potentially improve model efficiency by focusing on just the active regions during preprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Value Distribution Analysis\n",
    "\n",
    "Let's look at the distribution of non-zero DigiOccupancy values to understand the range and frequency of hit counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nAnalyzing and plotting value distributions...\")\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Flatten the arrays for histograms, EXCLUDING zeros\n",
    "run1_flat_nonzero = run1[run1 > 0].flatten()\n",
    "run2_flat_nonzero = run2[run2 > 0].flatten()\n",
    "\n",
    "print(f\"Number of non-zero entries in Run 1: {len(run1_flat_nonzero)}\")\n",
    "print(f\"Number of non-zero entries in Run 2: {len(run2_flat_nonzero)}\")\n",
    "\n",
    "# Check if there are non-zero entries before plotting\n",
    "if len(run1_flat_nonzero) == 0 and len(run2_flat_nonzero) == 0:\n",
    "    print(\"Warning: No non-zero values found in either dataset to plot distribution.\")\n",
    "else:\n",
    "    # If datasets are very large, sampling can speed up plotting significantly\n",
    "    sample_size = 500000 # Increased sample size a bit\n",
    "    if len(run1_flat_nonzero) > sample_size:\n",
    "        print(f\"Sampling {sample_size} non-zero values from Run 1 for histogram...\")\n",
    "        run1_plot_data = np.random.choice(run1_flat_nonzero, sample_size, replace=False)\n",
    "    else:\n",
    "        run1_plot_data = run1_flat_nonzero\n",
    "\n",
    "    if len(run2_flat_nonzero) > sample_size:\n",
    "        print(f\"Sampling {sample_size} non-zero values from Run 2 for histogram...\")\n",
    "        run2_plot_data = np.random.choice(run2_flat_nonzero, sample_size, replace=False)\n",
    "    else:\n",
    "        run2_plot_data = run2_flat_nonzero\n",
    "\n",
    "    # Determine common bins for comparison\n",
    "    combined_data = np.concatenate((run1_plot_data, run2_plot_data)) if len(run1_plot_data)>0 and len(run2_plot_data)>0 else (run1_plot_data if len(run1_plot_data)>0 else run2_plot_data)\n",
    "    if len(combined_data) > 0:\n",
    "        bin_edges = np.histogram_bin_edges(combined_data, bins=50)\n",
    "\n",
    "        # Create histograms with log scale on y-axis (count or density)\n",
    "        if len(run1_plot_data) > 0:\n",
    "            sns.histplot(run1_plot_data, bins=bin_edges, color='blue', label='Run 355456', alpha=0.6,\n",
    "                         log_scale=(False, True), stat='density') # Use density for comparison\n",
    "        if len(run2_plot_data) > 0:\n",
    "            sns.histplot(run2_plot_data, bins=bin_edges, color='orange', label='Run 357479', alpha=0.6,\n",
    "                         log_scale=(False, True), stat='density') # Use density for comparison\n",
    "\n",
    "        plt.xlabel('DigiOccupancy Value (for non-zero entries)')\n",
    "        plt.ylabel('Density (log scale)')\n",
    "        plt.title('Distribution of Non-Zero DigiOccupancy Values')\n",
    "        plt.legend()\n",
    "        plt.grid(axis='y', alpha=0.4) # Add horizontal grid lines\n",
    "        plt.tight_layout()\n",
    "        save_filename_dist = 'value_distribution.png'\n",
    "        print(f\"Saving value distribution plot to {save_filename_dist}\")\n",
    "        plt.savefig(save_filename_dist, dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"Skipping value distribution plot as no non-zero data available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Observations from Value Distribution:\n",
    "\n",
    "1. **Value Range**: DigiOccupancy values primarily range from around 100 to 1500, with the highest density around 900-1000.\n",
    "\n",
    "2. **Distribution Shape**: The distribution appears somewhat irregular with multiple peaks, suggesting complex underlying physical processes.\n",
    "\n",
    "3. **Run Differences**: The distributions show differences between the two runs, which is consistent with what we observed in the average occupancy maps and difference map.\n",
    "\n",
    "4. **Dynamic Range**: The log scale on the y-axis indicates a wide range in the frequency of different DigiOccupancy values, which may need to be addressed during preprocessing.\n",
    "\n",
    "5. **Normalization Need**: The wide range and irregular distribution of values suggests that normalization might be beneficial for model training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Total Activity Analysis\n",
    "\n",
    "Let's examine the total DigiOccupancy (sum of all hits) for each luminosity section to see if there are overall intensity differences between the runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nAnalyzing and plotting total hits per LS...\")\n",
    "# Calculate total hits per luminosity section (image)\n",
    "total_hits_run1 = np.sum(run1, axis=(1, 2))\n",
    "total_hits_run2 = np.sum(run2, axis=(1, 2))\n",
    "\n",
    "# Create figure\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot histograms\n",
    "# Determine common bins for better comparison\n",
    "combined_totals = np.concatenate((total_hits_run1, total_hits_run2))\n",
    "bins = np.histogram_bin_edges(combined_totals, bins=50)\n",
    "\n",
    "plt.hist(total_hits_run1, bins=bins, alpha=0.6, label='Run 355456', color='blue', density=True)\n",
    "plt.hist(total_hits_run2, bins=bins, alpha=0.6, label='Run 357479', color='orange', density=True) # Use density=True for shape comparison\n",
    "\n",
    "plt.xlabel('Total DigiOccupancy per Luminosity Section')\n",
    "plt.ylabel('Density') # Changed from Frequency to Density\n",
    "plt.title('Distribution of Total Hits per Luminosity Section')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "# Add summary statistics as text\n",
    "mean_run1 = np.mean(total_hits_run1)\n",
    "mean_run2 = np.mean(total_hits_run2)\n",
    "std_run1 = np.std(total_hits_run1)\n",
    "std_run2 = np.std(total_hits_run2)\n",
    "\n",
    "stats_text = (f\"Run 355456 Mean: {mean_run1:,.0f} (Std: {std_run1:,.0f})\\n\"\n",
    "              f\"Run 357479 Mean: {mean_run2:,.0f} (Std: {std_run2:,.0f})\")\n",
    "\n",
    "plt.figtext(0.65, 0.75, stats_text, # Adjust position as needed\n",
    "            bbox={\"facecolor\":\"white\", \"alpha\":0.8, \"pad\":5},\n",
    "            fontsize=11)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.97]) # Adjust layout to prevent overlap\n",
    "save_filename_totalhits = 'total_hits_distribution.png'\n",
    "print(f\"Saving total hits distribution plot to {save_filename_totalhits}\")\n",
    "plt.savefig(save_filename_totalhits, dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Practical Implications for ViT Model Design\n",
    "\n",
    "Based on our data exploration, here are some practical considerations for designing our Vision Transformer model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Insights from Data Analysis\n",
    "\n",
    "1. **Structure of Data**: The data shows highly structured patterns with activity concentrated in specific regions (iEta 0-12 and 50-60).\n",
    "\n",
    "2. **Distinguishing Features**: The main differences between runs appear to be in the intensity patterns within the active regions rather than their locations.\n",
    "\n",
    "3. **Symmetry**: There's a symmetrical pattern between the top and bottom regions, which may be leveraged in the model design.\n",
    "\n",
    "4. **High Sparsity**: The majority of cells show no activity (zeros), with activity confined to specific detector regions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing Strategies\n",
    "\n",
    "1. **Normalization**: Given the wide range of DigiOccupancy values, normalization will be important. Options include:\n",
    "   - Z-score normalization (subtract mean, divide by standard deviation)\n",
    "   - Min-max scaling to [0,1] range\n",
    "   - Log transformation to handle the wide dynamic range\n",
    "\n",
    "2. **Handling Sparsity**: The high sparsity of the data (many zeros) can be addressed by:\n",
    "   - Focusing on active regions only (cropping)\n",
    "   - Using attention mechanisms in the ViT to focus on active regions\n",
    "   - Applying thresholding to eliminate very low values that may be noise\n",
    "\n",
    "3. **Data Augmentation**: Since we have a fixed pattern with specific active regions, traditional image augmentations like rotations or flips would distort the physical meaning. Instead, we could consider:\n",
    "   - Adding small amounts of Gaussian noise to non-zero values\n",
    "   - Slight scaling of intensity values\n",
    "   - Randomly masking small regions to improve robustness\n",
    "\n",
    "4. **Feature Engineering**: We might consider:\n",
    "   - Calculating additional statistics per image (total hits, active cell count)\n",
    "   - Extracting active regions as separate features\n",
    "   - Converting to alternative representations that highlight the differences between runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vision Transformer Architecture Considerations\n",
    "\n",
    "1. **Patch Size**: Since the active regions show vertical striping patterns, we should choose a patch size that:\n",
    "   - Preserves the vertical structure\n",
    "   - Captures meaningful patterns without being too small\n",
    "   - A patch size of 4×4 or 8×8 might be appropriate, given the 64×72 image size\n",
    "\n",
    "2. **Attention Mechanisms**: The ViT's attention mechanism will be crucial for:\n",
    "   - Focusing on the active regions and ignoring inactive areas\n",
    "   - Capturing relationships between the top and bottom active bands\n",
    "   - Learning the subtle differences between runs in the intensity patterns\n",
    "\n",
    "3. **Model Complexity**: Given the structured nature of the data:\n",
    "   - A moderate-sized ViT should be sufficient (6-8 transformer blocks)\n",
    "   - Using multiple attention heads (8-12) to capture different aspects of the data\n",
    "   - MLP dimension should be large enough to represent the complex patterns\n",
    "\n",
    "4. **Mixture-of-Experts Option**: An MoE approach could be beneficial because:\n",
    "   - Different experts could specialize in different regions of the detector\n",
    "   - Some experts might focus on the top band, others on the bottom band\n",
    "   - This specialization could lead to better classification performance\n",
    "\n",
    "5. **Positional Embeddings**: Standard positional embeddings should work well as the position information is critical for understanding the spatial structure of the detector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary of Findings\n",
    "\n",
    "Based on our exploratory data analysis of the HCAL DigiOccupancy datasets, we can summarize our findings:\n",
    "\n",
    "1. **Data Structure**: Each dataset contains 10,000 images of size 64×72 representing hits in the HCAL detector.\n",
    "\n",
    "2. **Activity Pattern**: Both runs show a highly structured pattern with activity concentrated in two horizontal bands at iEta positions 0-12 and 50-60, with almost no activity in the middle regions.\n",
    "\n",
    "3. **Sparsity**: The data is extremely sparse, with activity confined to specific regions, which will require special handling during model development.\n",
    "\n",
    "4. **Distinguishing Features**: The primary differences between Run 355456 and Run 357479 are:\n",
    "   - Run 357479 generally has higher DigiOccupancy values in the middle of the active bands\n",
    "   - Run 355456 shows higher values at the edges of these bands\n",
    "   - These differences are consistent across all iPhi values\n",
    "\n",
    "5. **Classification Challenge**: The Vision Transformer will need to learn to focus on the subtle but consistent differences between runs while ignoring the inactive regions.\n",
    "\n",
    "6. **Value Distribution**: DigiOccupancy values span a wide range (from 0 to ~1500) with an irregular distribution, suggesting the need for appropriate normalization during preprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Next Steps\n",
    "\n",
    "With this understanding of the data, we can proceed to develop our Vision Transformer model. In the next notebook, we will:\n",
    "\n",
    "1. Implement data preprocessing based on our findings\n",
    "2. Design and implement a Vision Transformer architecture\n",
    "3. Train the model on our classification task\n",
    "4. Evaluate performance using accuracy, ROC curves, and AUC\n",
    "5. Experiment with both standard ViT and MoE-ViT approaches\n",
    "6. Analyze the model's attention patterns to understand what features it's using for classification\n",
    "\n",
    "The insights from this exploratory analysis will directly inform our modeling approach, particularly regarding handling of sparsity, choice of patch size, and attention mechanisms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- End of Data Exploration ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}